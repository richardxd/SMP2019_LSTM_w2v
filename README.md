# SMP2019_LSTM_w2v
This is a toy project where I classified SMP2019 data and trained a model to predict Chinese language. 

# Task description
The task here is to solve a classical supervised classification problem using Word2Vec/FastText with LSTM/GRU based neural network. There are 30 categories for different Chinese chat messages. My task is to freely explore different solutions and evaluate them using accuracies, precisions, and macro F1 score. 


# Method 1: Word Frequencies
The first method I tried is to initialize each word vector by the word's frequency in the entire document. The word vectors are not great because it does not capture the meaning of the word. For example, it does not demonstrate the numerical similarities between words. To solve this, I added an Embedding layer in my network to try to capture the similarities while training the neural network.

# Network: Simple LSTM
The structure of the network is the state-of-the-art LSTM model. Since the inputs are very sparse, I stick with a rather shallow LSTM model without too much layers. The LSTM layer captures the meaning of the words while taking into account of the position of the word with respect to the sentence without running into the vanishing gradients problem as an naive RNN would. To compensate for overfitting, I also added a few Dropout layers to randomly forget some of the input data. 

# Result:
We achieved 82.21% accuracy in clsssification in the Dev set. In addition, we have precision = 0.7533600233951127, recall score = 0.760744064682589, fbeta score = 0.7510238573390444. 

# Method 2: trainining the model with Word2Vec and Bidirectional LSTM.

The second method I tried is to resort to the help of gensim's word2vec algorithm to train the word vector using the document in order to improve the effectiveness of the word vectors.

# Network: Bidirectional LSTM
The structure of the network is still the LSTM model, except now the LSTM layer goes in forward and backward directions. This is because we want to handle cases like:
He said, "Teddy Roosevelt was the POTUS in the early 20th century."
vs
He said, "Teddy bears are fantastic."
where the word Teddy's meaning cannot be inferred without the later half of the sentence.

# Result:
We achieved 78.05% accuracy in clsssification in the Dev set. In addition, we have precision =0.7362174401838819, recall score = 0.7500266633457165, fbeta score = 0.7366286202207406. 


# Method 3: trainining the model with pretrained Chines word vector

Using the word vector generated by using gensim word2vec model, it seems like the accuracy decreases by 2-3%. I believed that the way that this may be due to the fact that the current size of the data is too small, so I decided to use pretrained embeddings from https://github.com/Embedding/Chinese-Word-Vectors.

# Network: Bidirectional LSTM
The structure of the network is still the LSTM model, except now the LSTM layer goes in forward and backward directions. This is because we want to handle cases like:
He said, "Teddy Roosevelt was the POTUS in the early 20th century."
vs
He said, "Teddy bears are fantastic."
where the word Teddy's meaning cannot be inferred without the later half of the sentence.

# Result:
Result: We achieved 93.25% accuracy in clsssification in the Dev set. In addition, we have (precision = 0.9190465450079812, recall score = 0.9300608772558282, fbeta score = 0.9216210986471332)
 
 
 # Summary
In our code we have adopted three different word vector generators and two different networks (the later networks are advanced version of the first version) to the problem. The first and the second models perform relatively bad due to the word vector's effectiveness as there are not too many input samples to learn while Chinese characters have vastly different meanings. The  model with pretrained word vectors performs a lot better because it is trained based on ngram2vec, which is the superset of word2vec and fasttext, and it has access to Weibo and Baidubaike, which are two popular Chinese version of Twitter and Wikipedia. 



Reference:
Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, Analogical Reasoning on Chinese Morphological and Semantic Relations, ACL 2018.

